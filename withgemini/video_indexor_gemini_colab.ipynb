{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfFIyW09wj5f"
      },
      "source": [
        "# Video Indexor (Gemini 2.5 Flash) – KAILASA PRO Version\n",
        "\n",
        "This notebook implements the Video Indexor using Gemini 2.5 Flash. It includes scene detection, frame extraction, KAILASA Ritual Intelligence analysis, transcription, and summarization.\n",
        "\n",
        "## Setup\n",
        "First, we install the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rdvQtwHwj5g"
      },
      "outputs": [],
      "source": [
        "!pip install google-genai opencv-python scenedetect[opencv] pillow tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC0L-61zwj5h"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKiMC9newj5h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import sqlite3\n",
        "import functools\n",
        "import random\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm  # Use notebook version of tqdm\n",
        "\n",
        "# PySceneDetect\n",
        "from scenedetect import open_video, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "\n",
        "# Gemini API (google-genai SDK)\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.api_core import exceptions as google_exceptions\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure Logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "log = logging.getLogger(\"video-indexor-pro-kailasa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JGgvi0kwj5i"
      },
      "source": [
        "## Configuration & Mount Drive\n",
        "Mount Google Drive to access your videos and save the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JzbOp4awj5i"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# CONFIGURATION\n",
        "DEFAULT_MODEL = \"gemini-2.5-flash\"\n",
        "MAX_FRAMES_PER_VIDEO = 20\n",
        "SCENE_THRESHOLD = 27\n",
        "MIN_SCENE_LENGTH = 15\n",
        "LONG_SCENE_THRESHOLD = 3.0  # seconds\n",
        "MAX_RES = 1024  # max dimension for frame downscale\n",
        "\n",
        "# Set your working directory here (where videos are located)\n",
        "# Examples: \"/content/drive/MyDrive/Videos\" or \"/content/drive/MyDrive/KailasaArchives\"\n",
        "WORK_DIR = \"/content/drive/MyDrive/VideoIndexer_Workspace\"\n",
        "DB_NAME = os.path.join(WORK_DIR, \"video_index.db\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "print(f\"Working Directory: {WORK_DIR}\")\n",
        "print(f\"Database Path: {DB_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXk5a3kWwj5j"
      },
      "source": [
        "## API Key Management\n",
        "We will attempt to load the API key from Colab Secrets (recommended) or fall back to manual input.\n",
        "Add a secret named `GEMINI_API_KEY` in the Colab secrets manager (key icon on the left)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e3D8Yfewj5j"
      },
      "outputs": [],
      "source": [
        "class KeyManager:\n",
        "    \"\"\"Manages API keys for the notebook context.\"\"\"\n",
        "\n",
        "    def __init__(self, key_list: List[str] = None):\n",
        "        self.keys: List[str] = []\n",
        "        self.current_index = 0\n",
        "\n",
        "        if key_list:\n",
        "            self.keys = key_list\n",
        "        else:\n",
        "            # Try loading from Colab Userdata\n",
        "            try:\n",
        "                keys_string = userdata.get('GEMINI_API_KEYS')\n",
        "                if keys_string:\n",
        "                    self.keys = keys_string.split(\",\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Fallback to manual input if still empty\n",
        "        if not self.keys:\n",
        "             print(\"No keys found in secrets. Please enter your Gemini API Key below:\")\n",
        "             manual_key = input(\"Enter API Key: \").strip()\n",
        "             if manual_key:\n",
        "                 self.keys = [manual_key]\n",
        "\n",
        "        if not self.keys:\n",
        "             # Last resort fallback (Not recommended for public notebooks)\n",
        "             self.keys = [\"AIzaSyDstjnyRAcgQ6bU3IdpTO7pZyZY1Lc6Ybg\"]\n",
        "             log.warning(\"Using default hardcoded key. Please provide your own key.\")\n",
        "\n",
        "    def get_client(self) -> genai.Client:\n",
        "        \"\"\"Get client for current key.\"\"\"\n",
        "        if not self.keys:\n",
        "            raise ValueError(\"No API keys available.\")\n",
        "\n",
        "        key = self.keys[self.current_index]\n",
        "        return genai.Client(api_key=key)\n",
        "\n",
        "    def rotate_key(self):\n",
        "        \"\"\"Rotate to next key.\"\"\"\n",
        "        if not self.keys:\n",
        "             raise ValueError(\"No API keys available during rotation.\")\n",
        "\n",
        "        prev_key = self.keys[self.current_index]\n",
        "        self.current_index = (self.current_index + 1) % len(self.keys)\n",
        "        new_key = self.keys[self.current_index]\n",
        "\n",
        "        masked_prev = f\"...{prev_key[-4:]}\" if len(prev_key) > 4 else \"current\"\n",
        "        masked_new = f\"...{new_key[-4:]}\" if len(new_key) > 4 else \"next\"\n",
        "        log.info(f\"Rotating key: {masked_prev} -> {masked_new}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp4S7UMuwj5j"
      },
      "source": [
        "## Utils & Retry Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xyCVaZAwj5j"
      },
      "outputs": [],
      "source": [
        "def retry_with_key_rotation(retries: int = 3, initial_delay: float = 2.0, backoff_factor: float = 2.0):\n",
        "    \"\"\"Decorator to retry with backoff AND key rotation on specific errors.\"\"\"\n",
        "    def decorator(func):\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(key_manager: 'KeyManager', *args, **kwargs):\n",
        "            delay = initial_delay\n",
        "            last_exception = None\n",
        "\n",
        "            for attempt in range(retries + 1):\n",
        "                try:\n",
        "                    # Get fresh client for this attempt\n",
        "                    client = key_manager.get_client()\n",
        "                    return func(client, *args, **kwargs)\n",
        "\n",
        "                except Exception as e:\n",
        "                    last_exception = e\n",
        "                    is_resource_exhausted = \"429\" in str(e) or \"Resource has been exhausted\" in str(e) or isinstance(e, google_exceptions.ResourceExhausted)\n",
        "\n",
        "                    if is_resource_exhausted:\n",
        "                        log.warning(f\"Resource Exhausted (429). Rotating key...\")\n",
        "                        key_manager.rotate_key()\n",
        "                        time.sleep(1.0)\n",
        "\n",
        "                    if attempt < retries:\n",
        "                        sleep_time = delay + random.uniform(0, 0.5)\n",
        "                        if not is_resource_exhausted:\n",
        "                             log.warning(f\"Error in {func.__name__}: {e}. Retrying in {sleep_time:.2f}s (Attempt {attempt + 1}/{retries})\")\n",
        "                        time.sleep(sleep_time)\n",
        "                        delay *= backoff_factor\n",
        "                    else:\n",
        "                        log.error(f\"Failed {func.__name__} after {retries} retries. Last error: {e}\")\n",
        "            raise last_exception\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "def strip_json_fences(text: str) -> str:\n",
        "    \"\"\"Remove accidental ```json fences from model outputs.\"\"\"\n",
        "    text = text.strip()\n",
        "    if text.startswith(\"```json\"):\n",
        "        text = text[len(\"```json\") :]\n",
        "    if text.startswith(\"```\"):\n",
        "        text = text[len(\"```\") :]\n",
        "    if text.endswith(\"```\"):\n",
        "        text = text[:-3]\n",
        "    return text.strip()\n",
        "\n",
        "def downscale_image(pil_img: Image.Image) -> Image.Image:\n",
        "    \"\"\"Resize image so that max dimension = MAX_RES (if needed).\"\"\"\n",
        "    w, h = pil_img.size\n",
        "    scale = MAX_RES / float(max(w, h))\n",
        "    if scale < 1.0:\n",
        "        return pil_img.resize((int(w * scale), int(h * scale)))\n",
        "    return pil_img\n",
        "\n",
        "def numpy_to_jpeg_bytes(np_img) -> bytes:\n",
        "    \"\"\"Convert numpy RGB image to downscaled JPEG bytes.\"\"\"\n",
        "    pil_img = Image.fromarray(np_img)\n",
        "    pil_img = downscale_image(pil_img)\n",
        "    buf = io.BytesIO()\n",
        "    pil_img.save(buf, format=\"JPEG\", quality=90)\n",
        "    return buf.getvalue()\n",
        "\n",
        "def calculate_file_hash(path: str, chunk_size: int = 8192) -> str:\n",
        "    \"\"\"Calculate SHA-256 hash of a file.\"\"\"\n",
        "    sha256 = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        while True:\n",
        "            chunk = f.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            sha256.update(chunk)\n",
        "    return sha256.hexdigest()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRfupPncwj5k"
      },
      "source": [
        "## Database Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R6IzbqYwj5k"
      },
      "outputs": [],
      "source": [
        "def init_db(db_path: str = None):\n",
        "    \"\"\"Initialize SQLite database and create tables.\"\"\"\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Videos table\n",
        "    c.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS videos (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            path TEXT UNIQUE NOT NULL,\n",
        "            file_hash TEXT,  -- SHA-256 hash of the video file\n",
        "            status TEXT NOT NULL,  -- NEW, PROCESSING, COMPLETED, FAILED, DUPLICATE\n",
        "            transcription TEXT,\n",
        "            summary_json TEXT,\n",
        "            frame_timestamps TEXT,  -- JSON list of timestamps to avoid re-detecting scenes\n",
        "            error_message TEXT,\n",
        "            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    try:\n",
        "        c.execute(\"ALTER TABLE videos ADD COLUMN file_hash TEXT\")\n",
        "    except sqlite3.OperationalError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        c.execute(\"ALTER TABLE videos ADD COLUMN frame_timestamps TEXT\")\n",
        "    except sqlite3.OperationalError:\n",
        "        pass\n",
        "\n",
        "    c.execute(\"CREATE INDEX IF NOT EXISTS idx_videos_file_hash ON videos(file_hash)\")\n",
        "\n",
        "    # Frames table\n",
        "    c.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS frames (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            video_id INTEGER,\n",
        "            timestamp REAL,\n",
        "            analysis_json TEXT,\n",
        "            FOREIGN KEY(video_id) REFERENCES videos(id)\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_video_record(path: str, db_path: str = None) -> Optional[Tuple]:\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT id, status FROM videos WHERE path = ?\", (path,))\n",
        "    row = c.fetchone()\n",
        "    conn.close()\n",
        "    return row\n",
        "\n",
        "def start_video_processing(path: str, file_hash: str, db_path: str = None) -> int:\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT id FROM videos WHERE path = ?\", (path,))\n",
        "    row = c.fetchone()\n",
        "\n",
        "    if row:\n",
        "        video_id = row[0]\n",
        "        c.execute(\"UPDATE videos SET status = 'PROCESSING', file_hash = ?, error_message = NULL, last_updated = CURRENT_TIMESTAMP WHERE id = ?\", (file_hash, video_id))\n",
        "    else:\n",
        "        c.execute(\"INSERT INTO videos (path, file_hash, status) VALUES (?, ?, 'PROCESSING')\", (path, file_hash))\n",
        "        video_id = c.lastrowid\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return video_id\n",
        "\n",
        "def update_video_timestamps(video_id: int, timestamps: List[float], db_path: str = None):\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"UPDATE videos SET frame_timestamps = ? WHERE id = ?\", (json.dumps(timestamps), video_id))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_stored_timestamps(video_id: int, db_path: str = None) -> Optional[List[float]]:\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT frame_timestamps FROM videos WHERE id = ?\", (video_id,))\n",
        "    row = c.fetchone()\n",
        "    conn.close()\n",
        "    if row and row[0]:\n",
        "        try: return json.loads(row[0])\n",
        "        except json.JSONDecodeError: return None\n",
        "    return None\n",
        "\n",
        "def get_existing_frame_analysis(video_id: int, timestamp: float, db_path: str = None) -> Optional[Dict[str, Any]]:\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT analysis_json FROM frames WHERE video_id = ? AND timestamp BETWEEN ? AND ?\",\n",
        "              (video_id, timestamp - 0.001, timestamp + 0.001))\n",
        "    row = c.fetchone()\n",
        "    conn.close()\n",
        "    if row and row[0]:\n",
        "        try: return json.loads(row[0])\n",
        "        except json.JSONDecodeError: return None\n",
        "    return None\n",
        "\n",
        "def save_frame_result(video_id: int, timestamp: float, data: Dict[str, Any], db_path: str = None):\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"INSERT INTO frames (video_id, timestamp, analysis_json) VALUES (?, ?, ?)\", (video_id, timestamp, json.dumps(data)))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def complete_video_processing(video_id: int, transcription: str, summary: Dict[str, Any], db_path: str = None):\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"UPDATE videos SET status = 'COMPLETED', transcription = ?, summary_json = ?, last_updated = CURRENT_TIMESTAMP WHERE id = ?\", (transcription, json.dumps(summary), video_id))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def fail_video_processing(video_id: int, error: str, db_path: str = None):\n",
        "    if db_path is None:\n",
        "        db_path = DB_NAME\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"UPDATE videos SET status = 'FAILED', error_message = ?, last_updated = CURRENT_TIMESTAMP WHERE id = ?\", (error, video_id))\n",
        "    conn.commit()\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0x4-5_wwj5k"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzEmSjIawj5k"
      },
      "outputs": [],
      "source": [
        "def build_frame_prompt() -> str:\n",
        "    return \"\"\"\n",
        "You are operating in **KAILASA Ritual Intelligence Mode**.\n",
        "Analyze this single video frame and return **ONLY ONE JSON OBJECT** with the\n",
        "following structure (no explanation text, no Markdown, no comments):\n",
        "\n",
        "{\n",
        "  \"scene_description\": \"What is happening in the frame in clear natural language\",\n",
        "  \"objects\": [\"list of key visible objects\"],\n",
        "  \"persons\": [\"short descriptions of each visible person or group\"],\n",
        "  \"activities\": [\"concise phrases describing visible actions\"],\n",
        "  \"environment\": \"indoor/outdoor/temple/ashram/stage/other\",\n",
        "  \"lighting\": \"low/medium/high/mixed\",\n",
        "  \"text_overlays\": [\"any visible on-screen text, subtitles or banners\"],\n",
        "  \"is_key_moment\": \"yes/no\",\n",
        "  \"key_moment_reason\": \"why this frame may be a key moment (or \\\"not a key moment\\\")\",\n",
        "\n",
        "  \"kailasa_ritual_intel\": {\n",
        "    \"is_kailasa_context\": \"yes/no/uncertain\",\n",
        "\n",
        "    \"deity_layer\": {\n",
        "      \"has_deity\": \"yes/no/uncertain\",\n",
        "      \"deities_present\": [\n",
        "        \"Names or descriptions of deities if recognizable (e.g. Nithyanandeshwara, Ganesha, Venkateshwara, Devi, Shiva Linga)\"\n",
        "      ],\n",
        "      \"deity_representation_type\": \"murti/statue/painting/linga/flag/photo/none/other\",\n",
        "      \"deity_focal_point\": \"which deity or deity-group appears central in this frame, if any\"\n",
        "    },\n",
        "\n",
        "    \"ritual_layer\": {\n",
        "      \"is_ritual_happening\": \"yes/no/uncertain\",\n",
        "      \"ritual_general_type\": \"puja/homa/abhishekam/alankara/satsang/darshan/yajna/yoga/administrative/other/none/uncertain\",\n",
        "      \"ritual_specific_name\": \"specific ritual name if visible or inferable (e.g. \\\"Nithya Puja\\\", \\\"Pada Puja\\\", \\\"Rudrabhishekam\\\", \\\"Rajyabhishek Alankara\\\", or 'unknown')\",\n",
        "      \"ritual_stage\": \"start/middle/end/not-clear\",\n",
        "\n",
        "      \"is_sph_present\": \"yes/no/uncertain\",\n",
        "      \"sph_role\": \"main deity/priest/acharya/teacher/blesser/participant/on-screen-only/absent/uncertain\",\n",
        "      \"is_sph_doing_ritual\": \"yes/no/uncertain\",\n",
        "      \"sph_ritual_actions\": [\n",
        "        \"very short phrases of what SPH is doing in the ritual if applicable (e.g. 'offering flowers', 'pouring water', 'placing kumkum', 'giving darshan')\"]\n",
        "    },\n",
        "\n",
        "    \"sph_visual_profile\": {\n",
        "      \"is_sph_visible\": \"yes/no/uncertain\",\n",
        "      \"sph_location_in_frame\": \"center/left/right/background/foreground/not-visible/uncertain\",\n",
        "\n",
        "      \"alankara_class\": \"simple-sannyasi/temple-puja/royal-alankara/yoga-based/event-specific/unknown\",\n",
        "      \"alankara_description\": \"detailed description of SPH's alankara – clothing, shawls, jewelry, headgear, sacred marks, garlands, etc.\",\n",
        "\n",
        "      \"robes_color\": \"dominant visible colors of SPH's clothing/robes\",\n",
        "      \"rudraksha_description\": \"description of rudraksha malas or beads if visible (layers, size, placement)\",\n",
        "      \"jewelry_highlights\": [\"key jewelry items (e.g. crown, mala, armlets, bracelets, earrings)\"] ,\n",
        "\n",
        "      \"throne_or_seat_type\": \"golden-throne/simhasana/peetha/regular-chair/floor-asana/sofa/other/none/uncertain\",\n",
        "      \"throne_or_seat_description\": \"visual description of the throne or seat on which SPH is sitting or standing near if applicable\"\n",
        "    },\n",
        "\n",
        "    \"sph_state_layer\": {\n",
        "      \"energetic_mode\": \"one of: upanishadic-silence/satsang-teaching/shakti-darshan/healing-blessing/ritual-performing/administrative/casual-interaction/unknown\",\n",
        "      \"conscious_state_hint\": \"visual hints of SPH's inner state (e.g. deep samadhi, intense teaching, gentle blessing, ferocious compassion)\",\n",
        "      \"emotional_state\": \"dominant observed emotion (e.g. compassion, ferocious-compassion, joy, humor, neutrality, focused-attention, serene, unknown)\",\n",
        "      \"crowd_response\": \"how people around are responding, if visible (e.g. receiving darshan, listening, prostrating, chanting)\"\n",
        "    },\n",
        "\n",
        "    \"sph_posture_layer\": {\n",
        "      \"body_posture\": \"standing/walking/cross-legged-on-floor/cross-legged-on-throne/sitting-on-chair/leaning/bowing/other/unknown\",\n",
        "      \"leg_posture\": \"if seated, describe leg arrangement (e.g. padmasana, ardha-padmasana, sukhasana, feet-down-on-floor, not-visible)\",\n",
        "      \"hand_mudras\": [\n",
        "        \"names of clear mudras if recognizable (e.g. abhaya mudra, varada mudra) or short descriptive phrases (e.g. 'hands folded in namaskar', 'both hands blessing', 'holding kamandalu')\"\n",
        "      ],\n",
        "      \"gaze_direction\": \"towards-camera/left/right/up/down/eyes-closed/not-clear\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "Important instructions:\n",
        "- Always return **valid JSON** that can be parsed directly.\n",
        "- If some information is not visible, fill the field with a clear string like \"not-visible\", \"none\", or \"unknown\" instead of leaving it empty or null.\n",
        "- Do NOT add any keys beyond what is defined above.\n",
        "- Do NOT wrap the JSON in Markdown fences.\n",
        "\"\"\"\n",
        "\n",
        "def build_summary_prompt(frame_json: List[Dict[str, Any]], transcription: str) -> str:\n",
        "    return f\"\"\"\n",
        "Create a structured JSON summary for the entire video.\n",
        "\n",
        "You are given:\n",
        "- A list of per-frame analysis JSON objects, many of which include KAILASA Ritual Intelligence information.\n",
        "- A transcription excerpt for the video's audio (may be empty or partial).\n",
        "\n",
        "Frame observations (sample up to 10 frames):\n",
        "{json.dumps(frame_json[:10], indent=2)}\n",
        "\n",
        "Transcription excerpt (may be empty):\n",
        "{transcription[:1600]}\n",
        "\n",
        "Return ONLY ONE JSON OBJECT with this structure (no Markdown):\n",
        "\n",
        "{{\n",
        "  \"title\": \"Short descriptive title for this video\",\n",
        "  \"summary\": \"2–3 paragraph narrative summary in plain English\",\n",
        "  \"key_events\": [\"important events in rough chronological order\"],\n",
        "  \"detected_objects\": [\"distinct important objects across frames\"],\n",
        "  \"detected_persons\": [\"distinct visible roles or persons (e.g. SPH, devotees, priests, musicians)\"],\n",
        "  \"overall_environment\": \"overall environment inference (e.g. main-temple, outdoor-stage, satsang-hall)\",\n",
        "  \"lighting_profile\": \"overall lighting style (e.g. bright daylight, indoor stage lighting, dim ritual lighting)\",\n",
        "  \"recommended_tags\": [\"tags useful for searchability\", \"like: SPH, puja, homa, satsang, darshan\"],\n",
        "\n",
        "  \"kailasa_summary\": {{\n",
        "    \"main_deities\": [\"primary deities present across the video if identifiable\"],\n",
        "    \"primary_rituals\": [\"main rituals or event types in this video (e.g. Nithya Puja, Pada Puja, Satsang, Shakti Darshan)\"],\n",
        "    \"sph_presence_summary\": \"never-present/present-in-some-frames/present-throughout/uncertain\",\n",
        "    \"sph_roles\": [\"summary of SPH's roles across the video (e.g. guru-teacher, ritual-performer, blesser, administrative-head)\"],\n",
        "    \"sph_energetic_modes\": [\"set of energetic modes observed (e.g. upanishadic-silence, satsang-teaching, shakti-darshan)\"],\n",
        "    \"sph_emotional_states\": [\"dominant emotional tones observed (e.g. compassion, ferocious-compassion, joy, serene)\",],\n",
        "    \"throne_and_seating_patterns\": \"short narrative of what SPH usually sits on or stands near throughout the video\",\n",
        "    \"notable_mudras\": [\"list of mudras or blessing gestures that appear repeatedly or are especially significant\"]\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Important:\n",
        "- Only use information that is reasonably supported by the frame JSON or transcription.\n",
        "- If some aspect is unclear, mark it as \"unknown\" rather than guessing.\n",
        "- Return **valid JSON** only, with no Markdown fences and no extra keys.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX2ovbHcwj5l"
      },
      "source": [
        "## Scene Detection & Frame Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAQgeKOBwj5l"
      },
      "outputs": [],
      "source": [
        "def detect_scenes(video_path: str):\n",
        "    \"\"\"Run PySceneDetect to identify scene boundaries.\"\"\"\n",
        "    video = open_video(video_path)\n",
        "    manager = SceneManager()\n",
        "    manager.add_detector(\n",
        "        ContentDetector(\n",
        "            threshold=SCENE_THRESHOLD,\n",
        "            min_scene_len=MIN_SCENE_LENGTH,\n",
        "        )\n",
        "    )\n",
        "    manager.detect_scenes(video)\n",
        "    return manager.get_scene_list()\n",
        "\n",
        "def get_video_timestamps(video_path: str, max_frames: int = MAX_FRAMES_PER_VIDEO) -> List[float]:\n",
        "    \"\"\"Detect scenes and return list of timestamps to extract.\"\"\"\n",
        "    # Check if video file exists\n",
        "    if not os.path.exists(video_path):\n",
        "        log.error(f\"Video file not found: {video_path}\")\n",
        "        return []\n",
        "\n",
        "    log.info(f\"Detecting scenes for {video_path}\")\n",
        "    try:\n",
        "        scenes = detect_scenes(video_path)\n",
        "    except Exception as e:\n",
        "        log.error(f\"Scene detection failed: {e}. Using fallback.\")\n",
        "        scenes = []\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = float(total_frames) / float(fps)\n",
        "    cap.release()\n",
        "\n",
        "    if not scenes:\n",
        "        timestamps = [0.0]\n",
        "        if duration > 2.0:\n",
        "            timestamps += [duration / 2.0, max(duration - 1.0, 0.0)]\n",
        "        log.info(f\"No scenes detected. Using fallback timestamps {timestamps}\")\n",
        "        return timestamps\n",
        "\n",
        "    timestamps = []\n",
        "    extracted = 0\n",
        "\n",
        "    for start_tc, end_tc in scenes:\n",
        "        if extracted >= max_frames:\n",
        "            break\n",
        "\n",
        "        start_time = float(start_tc.get_seconds() if hasattr(start_tc, \"get_seconds\") else start_tc)\n",
        "        end_time = float(end_tc.get_seconds() if hasattr(end_tc, \"get_seconds\") else end_tc)\n",
        "\n",
        "        timestamps.append(start_time)\n",
        "        extracted += 1\n",
        "\n",
        "        # Middle frame for long scenes\n",
        "        if end_time - start_time > LONG_SCENE_THRESHOLD and extracted < max_frames:\n",
        "            mid = start_time + (end_time - start_time) / 2.0\n",
        "            timestamps.append(mid)\n",
        "            extracted += 1\n",
        "\n",
        "    timestamps.sort()\n",
        "    return timestamps\n",
        "\n",
        "def extract_frames_at_timestamps(video_path: str, timestamps: List[float]) -> List[Tuple[Any, float]]:\n",
        "    \"\"\"Extract frames at specific timestamps.\"\"\"\n",
        "    if not timestamps:\n",
        "        return []\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "\n",
        "    frames: List[Tuple[Any, float]] = []\n",
        "\n",
        "    for ts in timestamps:\n",
        "        frame_no = int(ts * fps)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
        "        ok, frame = cap.read()\n",
        "        if ok and frame is not None:\n",
        "            frames.append((cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), ts))\n",
        "        else:\n",
        "            log.warning(f\"Could not read frame at {ts}s\")\n",
        "\n",
        "    cap.release()\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxVc2ZmTwj5l"
      },
      "source": [
        "## API Calling Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e0bYkaRwj5l"
      },
      "outputs": [],
      "source": [
        "@retry_with_key_rotation(retries=5)\n",
        "def analyze_frame(client: genai.Client, model_name: str, frame_np, timestamp: float) -> Dict[str, Any]:\n",
        "    \"\"\"Analyze a single frame with Gemini (sequential, with retries).\"\"\"\n",
        "\n",
        "    jpeg = numpy_to_jpeg_bytes(frame_np)\n",
        "\n",
        "    contents = types.Content(\n",
        "        parts=[\n",
        "            types.Part.from_text(text=build_frame_prompt()),\n",
        "            types.Part.from_bytes(data=jpeg, mime_type=\"image/jpeg\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=model_name,\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(temperature=0.2),\n",
        "    )\n",
        "\n",
        "    raw = response.text or \"\"\n",
        "    trimmed = strip_json_fences(raw)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(trimmed)\n",
        "    except Exception:\n",
        "        data = {\"error\": \"Invalid JSON returned by model\", \"raw\": trimmed}\n",
        "\n",
        "    data[\"timestamp_sec\"] = float(timestamp)\n",
        "    return data\n",
        "\n",
        "@retry_with_key_rotation(retries=5)\n",
        "def transcribe_video(client: genai.Client, model_name: str, video_path: str) -> str:\n",
        "    \"\"\"Upload → poll for processing → single-shot transcription.\"\"\"\n",
        "    try:\n",
        "        # Use only basename for file upload to avoid permission issues with Drive paths in some contexts\n",
        "        # But with google-genai SDK 0.6+ we can upload file-like objects or paths.\n",
        "        # The SDK handles reading the file.\n",
        "        up = client.files.upload(file=video_path)\n",
        "\n",
        "        # Poll until file is ready or failed\n",
        "        while getattr(up, \"state\", None) and getattr(up.state, \"name\", None) == \"PROCESSING\":\n",
        "            time.sleep(2.0)\n",
        "            up = client.files.get(name=up.name)\n",
        "\n",
        "        if getattr(up, \"state\", None) and getattr(up.state, \"name\", None) == \"FAILED\":\n",
        "            raise RuntimeError(\"Transcription failed (file processing failed)\")\n",
        "\n",
        "        contents = types.Content(\n",
        "            parts=[\n",
        "                types.Part.from_uri(file_uri=up.uri, mime_type=up.mime_type),\n",
        "                types.Part.from_text(\n",
        "                    text=(\n",
        "                        \"Transcribe the audio of this video. Return plain text only. \"\n",
        "                        \"If there are Sanskrit or other Indian-language chants, mantras, \"\n",
        "                        \"or songs, transcribe them as faithfully as possible, and add a \"\n",
        "                        \"simple English paraphrase in-line in brackets where appropriate.\"\n",
        "                    )\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        resp = client.models.generate_content(\n",
        "            model=model_name,\n",
        "            contents=contents,\n",
        "            config=types.GenerateContentConfig(temperature=0.1),\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            client.files.delete(name=up.name)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return (resp.text or \"\").strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "\n",
        "@retry_with_key_rotation(retries=5)\n",
        "def summarize_video(\n",
        "    client: genai.Client,\n",
        "    model_name: str,\n",
        "    frame_json: List[Dict[str, Any]],\n",
        "    transcription: str,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Summarize the entire video using per-frame analysis + transcription.\"\"\"\n",
        "\n",
        "    prompt = build_summary_prompt(frame_json, transcription)\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=model_name,\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(temperature=0.3),\n",
        "    )\n",
        "\n",
        "    raw = strip_json_fences(resp.text or \"\")\n",
        "\n",
        "    try:\n",
        "        return json.loads(raw)\n",
        "    except Exception:\n",
        "        raise ValueError(f\"Invalid summary JSON received: {raw[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klYPHJpwwj5m"
      },
      "source": [
        "## Processing Orchestration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p04Fpcgwj5m"
      },
      "outputs": [],
      "source": [
        "def process_video(\n",
        "    key_manager: KeyManager,\n",
        "    model_name: str,\n",
        "    video_path: str,\n",
        "    skip_transcript: bool = False,\n",
        "    skip_summary: bool = False,\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    # Check DB status\n",
        "    record = get_video_record(video_path)\n",
        "    if record:\n",
        "        vid_id, status = record\n",
        "        if status == \"COMPLETED\":\n",
        "            log.info(f\"Skipping {video_path} (already COMPLETED in DB)\")\n",
        "            # Retrieve result from DB/File if needed, or just return empty to signal skip\n",
        "            return {}\n",
        "\n",
        "    # Calculate hash\n",
        "    log.info(f\"Calculating hash for {video_path}...\")\n",
        "    file_hash = calculate_file_hash(video_path)\n",
        "\n",
        "    # Check for duplicates\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"\"\"\n",
        "        SELECT id, transcription, summary_json\n",
        "        FROM videos\n",
        "        WHERE file_hash = ? AND status = 'COMPLETED' AND path != ?\n",
        "        LIMIT 1\n",
        "    \"\"\", (file_hash, video_path))\n",
        "    duplicate = c.fetchone()\n",
        "    conn.close()\n",
        "\n",
        "    if duplicate:\n",
        "        orig_id, orig_transcript, orig_summary = duplicate\n",
        "        log.info(f\"Duplicate detected! Match with video ID {orig_id}. Skipping reprocessing.\")\n",
        "        return {}\n",
        "\n",
        "    log.info(f\"Processing video: {video_path}\")\n",
        "    video_id = start_video_processing(video_path, file_hash)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Resume or Detect Timestamps\n",
        "        stored_timestamps = get_stored_timestamps(video_id)\n",
        "        if stored_timestamps:\n",
        "            log.info(f\"Resuming with {len(stored_timestamps)} cached frame timestamps.\")\n",
        "            timestamps = stored_timestamps\n",
        "        else:\n",
        "            timestamps = get_video_timestamps(video_path)\n",
        "            update_video_timestamps(video_id, timestamps)\n",
        "\n",
        "        # Step 2: Analysis Loop\n",
        "        frame_results: List[Dict[str, Any]] = []\n",
        "        timestamps_to_extract = []\n",
        "\n",
        "        cached_analyses = {}\n",
        "        for ts in timestamps:\n",
        "            existing = get_existing_frame_analysis(video_id, ts)\n",
        "            if existing:\n",
        "                cached_analyses[ts] = existing\n",
        "                frame_results.append(existing)\n",
        "            else:\n",
        "                timestamps_to_extract.append(ts)\n",
        "\n",
        "        if timestamps_to_extract:\n",
        "            log.info(f\"Extracting {len(timestamps_to_extract)} new frames.\")\n",
        "            new_frames = extract_frames_at_timestamps(video_path, timestamps_to_extract)\n",
        "\n",
        "            for frame_np, ts in tqdm(new_frames, desc=\"Analyzing new frames\"):\n",
        "                analysis = analyze_frame(key_manager, model_name, frame_np, ts)\n",
        "                frame_results.append(analysis)\n",
        "                save_frame_result(video_id, ts, analysis)\n",
        "                time.sleep(0.3)\n",
        "        else:\n",
        "            log.info(\"All frames already analyzed.\")\n",
        "\n",
        "        frame_results.sort(key=lambda x: x.get(\"timestamp_sec\", 0.0))\n",
        "\n",
        "        transcription = \"\"\n",
        "        if not skip_transcript:\n",
        "            log.info(\"Transcribing audio…\")\n",
        "            try:\n",
        "                transcription = transcribe_video(key_manager, model_name, video_path)\n",
        "            except Exception as e:\n",
        "                log.error(f\"Transcription failed: {e}\")\n",
        "                transcription = f\"[Transcription Failed: {e}]\"\n",
        "\n",
        "        summary: Dict[str, Any] = {}\n",
        "        if not skip_summary:\n",
        "            log.info(\"Generating summary…\")\n",
        "            try:\n",
        "                summary = summarize_video(key_manager, model_name, frame_results, transcription)\n",
        "            except Exception as e:\n",
        "                log.error(f\"Summary failed: {e}\")\n",
        "                summary = {\"error\": f\"Summary generation failed: {e}\"}\n",
        "\n",
        "        output = {\n",
        "            \"video\": video_path,\n",
        "            \"frames\": frame_results,\n",
        "            \"transcription\": transcription,\n",
        "            \"summary\": summary,\n",
        "        }\n",
        "\n",
        "        out_path = Path(video_path).with_suffix(\".analysis.json\")\n",
        "        out_path.write_text(json.dumps(output, indent=2, ensure_ascii=False))\n",
        "        log.info(f\"Saved analysis → {out_path}\")\n",
        "\n",
        "        complete_video_processing(video_id, transcription, summary)\n",
        "        log.info(f\"Marked {video_path} as COMPLETED\")\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to process {video_path}: {e}\")\n",
        "        fail_video_processing(video_id, str(e))\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogygRXG7wj5m"
      },
      "source": [
        "## Run Processing\n",
        "Run the following cell to start processing videos in the `WORK_DIR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNSKbn-Iwj5n"
      },
      "outputs": [],
      "source": [
        "# Initialize DB\n",
        "init_db()\n",
        "\n",
        "# Initialize Key Manager\n",
        "# Note: Ensure you have added your API Key to the secrets or provided it above\n",
        "key_manager = KeyManager()\n",
        "\n",
        "work_path = Path(WORK_DIR)\n",
        "videos = sorted(list(work_path.glob(\"*.mp4\")) + list(work_path.glob(\"*.mkv\")))\n",
        "\n",
        "print(f\"Found {len(videos)} videos in {WORK_DIR}\")\n",
        "\n",
        "for index, video_file in enumerate(videos):\n",
        "    print(f\"\\n[{index+1}/{len(videos)}] Start: {video_file.name}\")\n",
        "    try:\n",
        "        process_video(\n",
        "            key_manager=key_manager,\n",
        "            model_name=DEFAULT_MODEL,\n",
        "            video_path=str(video_file)\n",
        "        )\n",
        "        print(f\"[{index+1}/{len(videos)}] Done: {video_file.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[{index+1}/{len(videos)}] Failed: {video_file.name} - {e}\")\n",
        "        # Continue to next video\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}